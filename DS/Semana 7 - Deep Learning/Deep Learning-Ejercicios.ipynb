{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es más fácil para ustedes?\n",
    "¿Contar cuántas personas hay en una foto, o realizar la operación mostrada?\n",
    "\n",
    "¿Cuál acción es más fácil para una computadora?\n",
    "\n",
    "A | B\n",
    "- | - \n",
    "![¿Cuántas personas hay en la foto?](images/count-people.PNG) | ![¿Cuál es el resultado de la operación](images/add-numbers.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales son una implementación computacional que buscan replicar el comportamiento de nuestro cerebro para poder reconocer patrones. \n",
    "\n",
    "El cerebro está compuesto por aproximadamente 10 mil millones de neuronas interconectadas entre si. \n",
    "\n",
    "Cada neurona está compuesta por tres partes principales:\n",
    "* El cuerpo de la célula neuronal (soma)\n",
    "* Conexiones de entrada (dendritas)\n",
    "* Conexiones de salida (axones)\n",
    "\n",
    "![neurona](images/neurona.PNG)\n",
    "\n",
    "Las neuronas se comunican entre sí enviándo pulsos electroquímicos que afectan la esctructura de cada célula.\n",
    "\n",
    "Los impulsos electroquímicos que las neuronas reciben en las dendritas pueden ser transmitidos a otras neuronas siempre y cuando el estímulo eléctrico sobrepase cierto umbral. \n",
    "\n",
    "El aprendizaje se da cuando se activan repetidamente ciertas neuronas, favoreciendo ciertas conexiones sobre otras. \n",
    "\n",
    "Aunque este modelo es muy simple para definir el comportamiento actual del cerebro humano, ha permitido que las computadoras puedan realizar tareas pudieran parecer simples como el reconocimiento de imágenes o predicción de eventos basada en la experiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconocer dígitos\n",
    "\n",
    "Por ejemplo, en la sigiente imagen fácilmente podemos reconocer que el número escrito es 504192.\n",
    "![504192](images/504192.png)\n",
    "\n",
    "Nosotros nos fijamos en las formas, trazos y direcciones para reconocer los dígitos, pero una computadora sólo sabe intepretar la información como una secuencia de números y no patrones. \n",
    "\n",
    "Entonces, para que una computadora aprenda a distinguir esta secuencia de números, por ejemplo, hay que 'enseñarle' con varios ejemplos cada uno de los números."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa las siguientes imágenes. ¿Crees que sería fácil para una computadora distinguir entre las dos entidades?\n",
    "\n",
    "¿Puedes clasificar correctamente cada imagen?\n",
    "\n",
    "A | B \n",
    "- | - \n",
    "![perros](images/sheepdog-or-mop.jpg) | ![chichuahua](images/dog-or-bagel.jpg) \n",
    "![chichuahua](images/owl-apple.jpg)   | \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué tareas puedo hacer con una red neuronal?\n",
    "\n",
    "Si contamos con suficientes datos, una red neuronal artificial puede ayudarnos a:\n",
    "* Identificar correos no deseados (spam).\n",
    "* Predecir si un cliente puede ser fraudulento o no.\n",
    "* Detectar la opinion de los clientes (positiva, negativa, neutra).\n",
    "* Identificar y reconocer los rostros de las personas.\n",
    "* Identificar objetos.\n",
    "* Reconocer voz (voz a texto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligencia Artificial contra el ser humano\n",
    "![leesedol](images/leesedol.PNG)\n",
    "[AlphaGo vs Lee Sedol](https://www.youtube.com/watch?v=IiWr6Wazm_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántas combinaciones posibles hay para armar el cubo rubik estándar (3x3)?\n",
    "\n",
    "![Rubik's combination count](images/rubiks-combi.PNG)\n",
    "\n",
    "<img src=\"images/rubikai.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "[Robot arma cubo de rubik en menos de 1 segundo](https://www.youtube.com/watch?v=by1yz7Toick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué es el Deep Learning? ¿Cuál es la diferencia con A.I. y M.L.?\n",
    "\n",
    "\n",
    "<img src=\"images/ai-vs-ml.png\" alt=\"Drawing\" style=\"width: 860px;\"/>\n",
    "\n",
    "[LINK: Diferencias entre AI,ML y DL](https://www.youtube.com/watch?v=KytW151dpqU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Por qué ahora?\n",
    "\n",
    "Las redes neuronales se han utilizado desde los años 50's, pero hay 3 factores principales que han influenciado en el crecimiento de las NN y DL.\n",
    "\n",
    "1. Big Data\n",
    "    * Grandes cantidades de información\n",
    "    * Recolección y almacenamiento.\n",
    "   \n",
    "\n",
    "2. Hardware\n",
    "    * Unides gráficas (GPU)\n",
    "    * Paralelización\n",
    "\n",
    "\n",
    "3. Algoritmos\n",
    "    * Mejores técnicas\n",
    "    * Nuevos modelos\n",
    "    * Toolbox y librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/timeline.png\" width = 250>\n",
    "\n",
    "<img align=\"right\" src=\"images/performances_vs_data.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Todos podemos aprender ML](https://www.youtube.com/watch?v=7ClLKBUvmRk4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal básica: perceptrón\n",
    "\n",
    "El modelo más simple de red neuronal consiste en tener una sola neurona. Este modelo se llama perceptrón y fue desarrollado por Frank Rosenblatt durante los 50's.\n",
    "\n",
    "El perceptrón toma como entrada varias señales (números), las procesa (operaciones matemáticas) y genera una salida (un número).\n",
    "\n",
    "<img src=\"images/input-outputs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "La neurona procesa las entradas de la siguiente manera:\n",
    "1. Cada entrada es multiplicada por un peso:\n",
    "\n",
    "$$x_1 \\rightarrow x_1 * w_1$$\n",
    "\n",
    "$$x_2 \\rightarrow x_2 * w_2$$\n",
    "\n",
    "2. Después, las entradas modificadas se suman y se agrega un término extra.\n",
    "\n",
    "$$z = (x_1 * w_1) + (x_2 * w_2) + b$$\n",
    "\n",
    "3. Finalmente, la suma se debe pasar a través de una función.\n",
    "\n",
    "$$a = f(z) = f(x_1 * w_1 + x_2 * w_2 + b)$$\n",
    "\n",
    "El perceptrón de Rosenblatt produce una salida entre `0` y `1`. Si la salida es 0, la neurona está inactiva, si el valor es 1, está completamente activa. \n",
    "\n",
    "La función que utiliza la neurona debe convertir cualquier valor posible al intervalo entre 0 y 1. Esta función se llama **función de activación**.\n",
    "\n",
    "Si el valor resultante de la suma $z$ es positivo, la neurona estará en un estado activo.\n",
    "\n",
    "Si el valor resultante de la suma $z$ es negativo, la neurona estará desactivada.\n",
    "\n",
    "En esencia, el perceptrón comprime un valor entre $-\\infty$ y $\\infty$ a uno de los valores $0$ y $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def step(z):\n",
    "    f = 1*(z>=0)\n",
    "    return f\n",
    "    \n",
    "x = np.arange(-10,10,0.1)\n",
    "y = step(x)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Función escalón', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:\n",
    "\n",
    "$w_1 = 3$\n",
    "\n",
    "$w_2 = -1$\n",
    "\n",
    "$x_1 = 3$\n",
    "\n",
    "$x_2 = 2$\n",
    "\n",
    "$b = -2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x_1 * w_1 + x_2 * x_2 + b$\n",
    "\n",
    "$z = 3*3 + 2*(-1) + (-2)$\n",
    "\n",
    "$z = 9 -2 -2$\n",
    "\n",
    "$z = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(z) = f(x_1 * w_1 + x_2 * x_2 + b) = f(5)$\n",
    "\n",
    "$f(z) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso de algebra lineal y cálculo\n",
    "\n",
    "## Definiciones\n",
    "\n",
    "Para calcular $z$ tenemos que calcular la multiplicación de cada una de las variables de entrada por su respectivo peso. \n",
    "\n",
    "Estas operaciones se pueden simplificar si utilizamos la notación de matrices y vectores.\n",
    "\n",
    "* Un escalar es un número, por ejemplo:   5\n",
    "\n",
    "\n",
    "* Un vector es un arreglo de **1 dimensión** de $n$ elementos. Por ejemplo, un vector de $n=4$ elementos: \n",
    "    \n",
    "    $\\begin{bmatrix} 1&2&3&4 \\end{bmatrix}$ \n",
    "    \n",
    "    Este es un vector fila (horizontal).\n",
    "    \n",
    "\n",
    "* Un vector columna es vertical:\n",
    "    \n",
    "    $\\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "* Una matriz es un arreglo de **2 dimensiones** de $m$ filas por $n$ columnas. Por ejemplo:\n",
    "    \n",
    "    $\\begin{bmatrix} 1&2&3 \\\\ 4&5&6 \\\\ 7&8&9 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las matrices y los vectores se pueden sumar y restar siempre que **tengan las mismas dimensiones**. La suma y la resta se realiza elemento a elemento.\n",
    "\n",
    "Por ejemplo, el vector fila de arriba tiene dimensiones $1\\times 4$ porque tiene una fila y 4 columnas.\n",
    "\n",
    "El vector columna de arriba es de dimensión $4\\times 1$ por que tienen 4 filas (o renglones) y 1 columna.\n",
    "\n",
    "¿Qué dimensiones tiene la matriz?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "(a) $\\begin{bmatrix} 1&2&3&4 \\end{bmatrix}$\n",
    "\n",
    "(b) $\\begin{bmatrix} 9&10&11 \\end{bmatrix}$\n",
    "\n",
    "(c) $\\begin{bmatrix} 2\\\\ 4\\\\ 6\\\\ 8 \\end{bmatrix}$\n",
    "\n",
    "(d) $\\begin{bmatrix} 1&2&3&4 \\\\ 5&6&7&8 \\\\ 9&10&11&12 \\end{bmatrix}$\n",
    "\n",
    "(e) $\\begin{bmatrix} 1\\\\ 3\\\\ 5\\\\ 7 \\end{bmatrix}$\n",
    "\n",
    "(f) $\\begin{bmatrix} 1&0&0&1\\end{bmatrix}$\n",
    "\n",
    "(g) $\\begin{bmatrix} 20&18\\\\ 16&14\\\\ 12&10 \\\\ 8&6 \\end{bmatrix}$\n",
    "\n",
    "(h) $\\begin{bmatrix} 1&0&1&0 \\\\ 0&1&0&1 \\\\ 1&0&1&0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles operaciones se pueden realizar? Si es posible realizar la operación , ¿cuál es el resultado?\n",
    "\n",
    "1. (a) + (b)\n",
    "2. (a) - (e)\n",
    "3. (a) + (f)\n",
    "4. (b) + (f)\n",
    "5. (c) - (e)\n",
    "6. (d) + (g)\n",
    "7. (g) + (h)\n",
    "8. (h) + (f)\n",
    "9. (d) - (h)\n",
    "10. (e) + (f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpuesta\n",
    "Hay una operación para voltear las dimensiones de un vector o una matriz, es decir, cambiar las filas por las columnas o viceversa. Esta operación se llama **transpuesta** de un vector o una matriz.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "$ (d)^T = \\begin{bmatrix} 1&2&3&4 \\\\ 5&6&7&8 \\\\ 9&10&11&12 \\end{bmatrix}^T = \\begin{bmatrix} 1&5&9 \\\\ 2&6&10 \\\\ 3&7&11 \\\\ 4&8&12 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las operaciones anteriores, ¿cuáles si se podrían realizar si se transpone uno de los elementos de la operación?\n",
    "\n",
    "1. (a) + (b)\n",
    "2. (a) - (e)\n",
    "3. (a) + (f)\n",
    "4. (b) + (f)\n",
    "5. (c) - (e)\n",
    "6. (d) + (g)\n",
    "7. (g) + (h)\n",
    "8. (h) + (f)\n",
    "9. (d) - (h)\n",
    "10. (e) + (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicación de vectores y matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los vectores y matrices no se multiplican como los números (escalares). Dos vectores se pueden multiplicar siempre y cuando tengan el mismo número de elementos. La esta multiplicación se llama **producto punto** o **producto escalar**.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "\n",
    "(a)$\\cdot$(f) = $\\begin{bmatrix} 1&2&3&4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1&0&0&1\\end{bmatrix}$\n",
    "\n",
    "\n",
    "(a)$\\cdot$(f) = $ 1\\times 1 + 2\\times 0 +  3\\times 0 + 4\\times 1$\n",
    "\n",
    "\n",
    "(a)$\\cdot$(f) = $ 1 + 0 + 0 + 4 = 5$\n",
    "\n",
    "El resultado del producto punto siempre es un escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un vector en Python utilizamos la librería `numpy`. Para crear vectores se utiliza `np.array` y para multiplicar vectores utilizamos `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "f = np.array([1,0,0,1])\n",
    "\n",
    "print('a=',a)\n",
    "print('f=',f)\n",
    "print('El producto punto de a y f es:', np.dot(a,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear una matriz en python, hay que pasar renglón por renglón a la función `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para python, los vectores no son ni filas ni columnas. Para poder decirle a python que queremos tratar a los vectores como filas y columnas, hay que tratarlos como matrices de $m \\times 1$ o $1\\times n$, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4]])\n",
    "c = np.array([[2],[4],[6],[8]])\n",
    "\n",
    "print('a es un vector fila:\\n', a, '\\n')\n",
    "print('c es un vector columna:\\n', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para multiplicar matrices hay que hacer unas cuantas operaciones más. Para poder hacer una multiplicación con matrices, el número de columnas del primer elemento debe ser igual al número de filas del segundo elemento.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "$\\begin{bmatrix} 1&2&3&4 \\\\ 5&6&7&8 \\\\ 9&10&11&12 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix}$ \n",
    "\n",
    "La primer matriz tiene **3** renglones y **4** columnas, y el segundo vector tiene **4** renglones y **1** columna. Como el número de columnas del primer elemento coincide con el número de renglones en el segundo elemento, la multiplicación se puede hacer.\n",
    "\n",
    "La multiplicación de matrices es hacer un **producto punto** de cada renglón del primer elemento con cada columna del segundo elemento.\n",
    "\n",
    "$\\begin{bmatrix} 1&2&3&4 \\\\ 5&6&7&8 \\\\ 9&10&11&12 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = \n",
    "    \\begin{bmatrix} [1\\quad2\\quad3\\quad4] \\cdot [1\\quad0\\quad0\\quad1] \\\\ [5\\quad6\\quad7\\quad8] \\cdot [1\\quad0\\quad1\\quad1] \\\\ [9\\quad10\\quad11\\quad12] \\cdot [1\\quad0\\quad0\\quad1] \\end{bmatrix} \n",
    "$ \n",
    "\n",
    "$\\begin{bmatrix} 1&2&3&4 \\\\ 5&6&7&8 \\\\ 9&10&11&12 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = \n",
    "    \\begin{bmatrix} \n",
    "    1\\times 1+2\\times 0+3\\times 0+ 4\\times 1\\\\\n",
    "    5\\times 1+ 6\\times 0+ 7\\times 0+ 8\\times 1\\\\\n",
    "    9\\times 1+ 10\\times 0+ 11\\times 0+ 12\\times 1\\\\   \n",
    "    \\end{bmatrix} = \\begin{bmatrix} 5\\\\ 13\\\\ 21\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "El resultado de la multiplicación tiene el mismo número de filas que el primer elemento y el mismo número de columnas que el segundo elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En python, para multiplicar matrices utilizamos el comando `np.matmul`, siempre y cuando las dimensiones de los parámetros sean las correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo, d x f\n",
    "np.matmul(d,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para multiplicar vectores fila y columna se debe seguir la misma regla que las matrices: el número de columnas del primer vector debe ser igual al número de filas del segundo.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "$f\\times e = \\begin{bmatrix} 1&0&0&1\\end{bmatrix} \\times \\begin{bmatrix} 1\\\\ 3\\\\ 5\\\\ 7 \\end{bmatrix}$\n",
    "\n",
    "$f\\times e = 1\\times1 + 0\\times3 + 0\\times5 + 1\\times7 = 8$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transponer una matriz o un vector en python se utiliza np.transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d, '\\n')\n",
    "print('La transpuesta de d es\\n', np.transpose(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo\n",
    "\n",
    "Para poder realizar el aprendizaje, los perceptrones y las redes neuronales necesitan encontrar los valores óptimos de los pesos. Son los pesos ($w$) los que determinan si la predicción es correcta o no. Para ello, recurrimos a una herramienta de las matemáticas: la optimización.\n",
    "\n",
    "Supongamos que tenemos una relación entre la variable $x$ y la variable $y$\n",
    "\n",
    "$y = x^2 + 2x -5$\n",
    "\n",
    "Queremos encontrar el valor de $x$ de tal manera que el valor de $y$ sea al más chico posible. La derivada de la función $y$ indica la **dirección** hacia donde hay más incremento, entonces, si multiplicamos la derivada por $-1$ nos indica la dirección hacia donde hay más decremento.\n",
    "\n",
    "$\\frac{dy}{dx} = 2x + 2$ (Dirección de incremento)\n",
    "\n",
    "$-\\frac{dy}{dx} = -2x - 2$ (Dirección de decremento)\n",
    "\n",
    "Los algoritmos de optimización utilizan la información de las derivadas disminuir el valor del error. Como se busca que el error sea lo más chico posible, utilizamos la dirección de decremento.\n",
    "\n",
    "El algoritmo básico para optimización se llama **Gradiente descendiente** (gradient descent)\n",
    "\n",
    "$ \\theta = \\theta - \\alpha \\text{ } d\\theta$\n",
    "\n",
    "Este algoritmo cambia el tamaño de los pesos usando la dirección donde se produce una disminución del error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo.\n",
    "x = np.arange(-10,10,0.1)\n",
    "\n",
    "def funcionPrueba(x):\n",
    "    return np.power(x,2) +2*x - 5\n",
    "\n",
    "\n",
    "plt.plot(x, funcionPrueba(x))\n",
    "plt.scatter(5, funcionPrueba(5), color='green')\n",
    "plt.scatter(-1, funcionPrueba(-1), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programar una neurona\n",
    "\n",
    "Volviendo al perceptron, podemos expresar las variables $x$ y los pesos $w$ como vectores. Lo más común es que los vectores sean representados como columnas. Entonces para hacer la multiplicación de los vectores es necesario calcular la transpuesta de uno de ellos.\n",
    "\n",
    "Volviendo al ejemplo anterior:\n",
    "\n",
    "$$w_1 = 3$$\n",
    "\n",
    "$$w_2 = -1$$\n",
    "\n",
    "$$x_1 = 3$$\n",
    "\n",
    "$$x_2 = 2$$\n",
    "\n",
    "$$b = -2$$\n",
    "\n",
    "Podemos escribir las operaciones como:\n",
    "\n",
    "$$x= \\begin{bmatrix} 3\\\\ 2\\end{bmatrix} = [3\\quad2]^T$$\n",
    "\n",
    "$$w= \\begin{bmatrix} 2 \\\\ -1\\end{bmatrix} = [2\\quad-1]^T$$\n",
    "\n",
    "$$b=-2$$\n",
    "\n",
    "La trasformación que hace el perceptrón la podemos escribir como:\n",
    "\n",
    "$$y = f(z) = f(w^Tx +b)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función sigmoide\n",
    "\n",
    "La función escalón tiene dos desventajas: no es una función continua y en las regiones donde sí lo es la derivada es 0. Esto es un impedimento puesto que para que la neurona pueda aprender necesita la información codificada en la derivada. Por ello, es conveniente utilizar una función que se asemeje a la función escalor pero que sea continua. Esta función existe y se le conoce como sigmoide o función logística.\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(z):\n",
    "    # COMPLETA EL CÓDIGO AQUI\n",
    "    \n",
    "    return g\n",
    "\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Función sigmoide', size=14)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "Crear una función `neurona` que haga el cálculo de una neurona con la función sigmoide como función de activación. La función debe recibir como parámetros vectores ``x, w`` y un escalar `b`, y debe regresar un valor `y` entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurona(x,w,b):\n",
    "    z = \n",
    "    y = \n",
    "    return y\n",
    "\n",
    "w = np.array([[3],[2]])\n",
    "x = np.array([[2],[-1]])\n",
    "b = -2\n",
    "\n",
    "neurona(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-5],[2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurona(x,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la función sigmoide, el perceptrón de Rosenblatt se parece a otro método de Machine Learning. ¿Cuál es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo perceptrón como regresión logística.\n",
    "\n",
    "Siendo idéntico a la regresión logística, el perceptrón funciona como un **clasificador binario**, es decir, dados varios datos de entrada, el perceptrón genera una probabilidad de que los dato pertenezcan a una categoría.\n",
    "\n",
    "El resultado de una regresión logística es un número entre 0 y 1. Para tomar la decisión de a qué categoría pertenece el resultado, hay que tomar un punto de referencia. Normalmente, los valores mayores o iguales a 0.5 se clasifican como 1 y en el caso contrario como 0.\n",
    "\n",
    "Observemos el conjunto de datos `Default`, el cual contiene registros de clientes de créditos. La columna `default` muestra si dejaron de pagar (True) o si terminaron de pagar (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "clientes = pd.read_csv('datasets/Default.csv').drop('Unnamed: 0', axis=1)\n",
    "clientes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables `student`, `balance` e  `income` se utilizaran para predecir si una persona va a dejar de pagar el crédito (`default`).\n",
    "\n",
    "La columna `student` es un indicador de la persona que es estudiante. Las columnas `balance` e `income` son datos relacionados con la capacidad de pago.\n",
    "\n",
    "Para que el perceptrón pueda predecir exitosamente si una persona va a dejar de pagar el crédito basado en las tres variables anteriores, hay que seleccionar los pesos (`w`) correctos.\n",
    "\n",
    "Como todo método de Machine Learning, hay que seleccionar una **función de costo** y un **método de optimización** para encontrar los pesos adecuados. \n",
    "\n",
    "Para medir que tan acertada es la predicción utilizamos la función de costo que en una regresión logística: \n",
    "    \n",
    "$J = \\frac{1}{m}\\sum_{i=0}^{m}-y^{i}\\cdot ln(A^{i}) - (1-y^{i})\\cdot ln(1-A^{i})$, \n",
    "\n",
    "donde $A = g(w^Tx+b)$ y $m$ es el número de muestras.\n",
    "\n",
    "\n",
    "<img src=\"images/ffperceptron.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón sólo acepta entradas numéricas, por lo que hay que codificar las variables categóricas. Además, hay que cambiar la escala de las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clientes.copy()\n",
    "data['default'] = \n",
    "data['student'] = \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data[['balance','income']] = \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar en train y test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "X_test  = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test  = X_test.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test  = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos los pesos con un valor aleatorio. Por esta ocasión los podemos inicializar en ceros.\n",
    "def inicializar_ceros(dim):\n",
    "    w = np.zeros([dim,1])\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular los pesos se necesitan las derivadas de la función de costo con respecto a $w$ y a $b$ para utilizar el **gradiente descendiente** (*gradient descent*) hacia la dirección de decremento para el error.\n",
    "\n",
    "La fórmula general del gradiente descendiente es:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\text{ } d\\theta$$\n",
    "\n",
    "Para el caso particular de $w$ y $b$, \n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X \\cdot (A-Y)^T$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (A-Y)$$\n",
    "\n",
    "\n",
    "\n",
    "Al método para actualizar los pesos se le conoce como **back propagation** (*propagación hacia atrás*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos el cálculo al pasar los datos a través del perceptrón.\n",
    "def propagar(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A =  \n",
    "    \n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) \n",
    "    \n",
    "    dw = \n",
    "    db = \n",
    "    \n",
    "    grads = {'dw' : dw,\n",
    "             'db' : db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizamos utilizando el gradiente descendiente\n",
    "def optimizar(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "      \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        grads, cost = propagar(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        w =  \n",
    "        b = \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que los pesos están calculados, hay que predecir el resultado.\n",
    "def predecir(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Calcular la activación con los pesos actuales\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    \n",
    "    # Punto de corte\n",
    "    cut = 0.2\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        #Predecir utilizando como punto de corte 0.25\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > cut else 0\n",
    "        \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntamos todo en un sola función\n",
    "def modelo(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    w, b = inicializar_ceros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimizar(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test  = predecir(w, b, X_test)\n",
    "    Y_prediction_train = predecir(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurona = modelo(X_train, y_train, X_test, y_test, \n",
    "                    num_iterations = 2000, \n",
    "                    learning_rate = 0.005, \n",
    "                    print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graficar cómo va disminuyendo el error.\n",
    "costs = np.squeeze(neurona['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Costo')\n",
    "plt.xlabel('Iteraciones (cada 100)')\n",
    "plt.title(\"Learning rate =\" + str(neurona[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(neurona['Y_prediction_train'].squeeze(), Y_train, rownames=['Predicción'], colnames=['Valor Real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales artificiales\n",
    "\n",
    "El perceptrón es un modelo muy simple que no puede predecir cuando los patrones en los datos son muy complejos. Sin embargo, nuestro cerebro no usa neuronas aisladas para procesar la información, sino que los impulsos eléctricos pasa a través de una serie de neuronas para producir una respuesta sensorial.\n",
    "\n",
    "## Nodos\n",
    "Inspirándose en el cerebro, entonces al interconectar varios perceptrones (nodos), se puede llegar a descifrar patrones más complejos en los datos.\n",
    "\n",
    "\n",
    "<img src=\"images/singlelayer.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "Las redes neuronales consisten en varias **capas** de neuronas, cada una con un número específico de **nodos**, los cuales procesan los datos para producir una salida en la última capa. Cada capa es independiente una de la otra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación\n",
    "Además, la función sigmoide no es la única que se puede utilizar como función de activación. Una función que ha ganado mucha popularidad en los últimos años es la función **ReLu** (*Rectified Linear Unit*). También está la función **tangente hiperbólica** que es parecida a la sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graficar funciones de activación más comunes\n",
    "x = np.arange(-5,5,0.1)\n",
    "s = 1/(1+np.exp(-x))\n",
    "t = np.tanh(x)\n",
    "r = x*(x>0)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(x,s, color='red')\n",
    "plt.axvline(0, linestyle='dashed', color='gray')\n",
    "plt.axhline(0, linestyle='dashed', color='gray')\n",
    "\n",
    "plt.ylim([-2,3])\n",
    "plt.title('Sigmoide')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(x,t)\n",
    "plt.axvline(0, linestyle='dashed', color='gray')\n",
    "plt.axhline(0, linestyle='dashed', color='gray')\n",
    "plt.ylim([-2,3])\n",
    "plt.title('Tangente hiperbólica')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(x,r, color='green')\n",
    "plt.axvline(0, linestyle='dashed', color='gray')\n",
    "plt.axhline(0, linestyle='dashed', color='gray')\n",
    "plt.ylim([-2,3])\n",
    "plt.title('ReLu')\n",
    "\n",
    "plt.suptitle('Funciones de activación', fontsize=16, weight='bold', y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas\n",
    "\n",
    "La primera capa de una red neuronal consiste en cada una de las variables de entrada. Esta capa se le denomina **input layer** (*capa de entrada*).\n",
    "\n",
    "Las capas que se encuentran en medio se les denomina **hidden layers** (capas ocultas).\n",
    "\n",
    "La última capa, que es la que entrega el resultado, se le denomina **output layer** (capa de salida).\n",
    "\n",
    "![archi](images/nnarchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de agregar capas es para que la red neuronal pueda ir descubriendo patrones simples en las primeras capas, para después ir combinándolos en patrones cada vez más complejos.\n",
    "\n",
    "![patterns](images/highfeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal con pocas capas (2-8) se le conoce como red **superficial**. Cuando el número de capas es grande (>8), entonces la red neuronal es **profunda**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de optimización.\n",
    "\n",
    "Además del *gradient descent*, existen distintos algoritmos que tienen eficiencias similares o mejores cuando hay grandes cantidades de datos. Cada uno tiene sus ventajas y desventajas.\n",
    "\n",
    "* Gradient descent variants:\n",
    "    1. Batch gradient descent\n",
    "    2. Stochastic gradient descent\n",
    "    3. Mini-batch gradient descent\n",
    "\n",
    "\n",
    "* Gradient descent optimization algorithms:\n",
    "    4. Momentum\n",
    "    5. Nesterov accelerated gradient\n",
    "    6. Adagrad\n",
    "    7. Adadelta\n",
    "    8. RMSprop\n",
    "    9. Adam\n",
    "    10. AdaMax\n",
    "\n",
    "Los optimizadores más comunes son **Mini-batch Gradient Descent** y **Adam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuciones de costo\n",
    "\n",
    "Las funciones de costo cambian dependiendo de tipo de problema. Si queremos predecir un número como en una regresión lineal o polinomial, entonces utilizamos:\n",
    "\n",
    "* Mean Squared Error (MSE)\n",
    "$$\\frac{1}{m} \\sum (y - \\hat{y})^2 $$\n",
    "\n",
    "* Mean Absolute Error (MAE)\n",
    "$$\\frac{1}{m} \\sum |y - \\hat{y}|$$\n",
    "\n",
    "\n",
    "\n",
    "Por otro lado, cuando realicemos una clasificación utilizamos `entropía cruzada`:\n",
    "\n",
    "* Binary cross entropy (Clasificación binaria)\n",
    "$$ -\\frac{1}{m} \\sum y\\cdot ln(a) + (1-y) \\cdot ln(1-a)$$\n",
    "\n",
    "\n",
    "* Multinomial cross entropy o Softmax (Clasificación multicategoría)\n",
    "$$ -\\frac{1}{m}\\sum_{c=1}^{N} \\sum_{i=1}^m y_{i, c}\\cdot ln(a_{i,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros\n",
    "\n",
    "El número de nodos en cada capa es una decisión de quien programa la red neuronal. Este es uno de los llamados hiperparámetros. Los hiperparámetros son valores que podemos ajustar cómo es que la red neuronal va aprender. Los hiperparámetros más comunes son:\n",
    "\n",
    "+ **Número de capas**\n",
    "\n",
    "+ **Número de nodos en cada capa**\n",
    "\n",
    "+ **Tamaño de lote** (batch size): Cuando calculamos los gradientes para mejorar los parámetros de la red neuronal, es costoso utilizar todos los datos si el dataset de entrada es muy grande. Por eso, en vez de utilizar todos los datos disponibles a la vez, se calculan los gradientes con pequeños lotes (mini-batches) de datos. \n",
    "\n",
    "+ **Número de épocas** (epochs): Cuando dividimos los datos en lotes, debemos considerar cuántas veces queremos que la red neuronal utilice todos los datos para entrenarse. Una época (epoch) es cuando todo el conjunto de datos para a través de la red neuronal 1 vez. \n",
    "\n",
    "+ **Métricas** : son cuantificaciones de qué tan bien el modelo puede predecir el resultado. Se utilizan con el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de dígitos con redes neuronales\n",
    "\n",
    "Un conjunto de datos muy famoso para aprender redes neuronales es el MNIST. Esta base de datos contiene 60,000 ejemplos de dígitos (0 al 9) escritos a mano y escaneados. El problema consiste en crear una red neuronal que sea capaz de reconocer los dígitos escritos.\n",
    "\n",
    "Todas las imágenes están en escala de grises y tienen un tamaño de 28 x 28 pixeles. \n",
    "\n",
    "\n",
    "<img src=\"images/mnist.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "No todos los dígitos son legibles incluso para los seres humanos:\n",
    "\n",
    "<img src=\"images/mnistnonread.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar, repacemos la terminología y algunos puntos importantes:\n",
    "\n",
    "* `x` se refieren a los datos de entrada: las variables que nos ayudarán a determinar nuestra predicción.\n",
    "\n",
    "* `y` es el nombre de las etiquetas o el valor que queremos predecir.\n",
    "\n",
    "* $\\hat{y}$ son las predicciones hechas por el modelo. Deben ser muy parecidos a los valores `y`.\n",
    "\n",
    "* Los datos de entrenamiento son los que utilizamos para encontrar el modelo de predicción. \n",
    "\n",
    "* Los datos de prueba los utilizamos para ver si el modelo entrenado es bueno o no.\n",
    "\n",
    "* La función de costo se utiliza para medir qué tan precisas son las predicciones del modelo.\n",
    "\n",
    "* El algoritmo de optimización controla cómo van variando los pesos del modelo para disminuir el error o la función de costo.\n",
    "\n",
    "* La cantidad de nodos de salida debe ser 1 si se hace una regresión o una clasificación binaria. Para clasificación multicategoría, la cantidad de nodos en la capa de salida debe ser igual al número de categorías.\n",
    "\n",
    "* Una capa es **densa** si todas las entradas están conectadas a todos los nodos de la capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolkits de Deep Learning\n",
    "\n",
    "Con el incremento de popularidad de las redes neuronales, se desarrollaron librerías y toolkits que funcionan en distintos lenguajes de programación pra implementar de manera fácil estos algoritmos. Entre los toolkits más famosos están:\n",
    "\n",
    "### Tensorflow\n",
    "TensorFlow es el toolkit de DL más famoso y más utilizado por el momento. Grandes compañías como Airbus, Twitter, IBM lo han utilizado por su gran flexibilidad. Fue desarrollado por Google y actualmente está distibuido como software de código abierto.\n",
    "\n",
    "<img src=\"images/tensorflow.png\" alt=\"Drawing\" style=\"height: 200px;\"/>\n",
    "\n",
    "### Pytorch\n",
    "Es una librería basada en Torch para el cómputo eficiente con tensores de alta dimensionalidad y para el desarrollo de redes neuronales profundas. Es utilizado por Facebook y ha tenido un crecimiento de popularidad importante en los últimos años.\n",
    "\n",
    "<img src=\"images/pytorch.jpeg\" alt=\"Drawing\" style=\"height: 100px;\"/>\n",
    "\n",
    "### Keras\n",
    "Keras es una librería para crear redes neuronales, convolucionales y recurrentes de manera rápida y simple. Esta librería puede utilzar Tensorflow of Theano como motor de cómputo. \n",
    "\n",
    "<img src=\"images/keras.png\" alt=\"Drawing\" style=\"height: 80px;\"/>\n",
    "\n",
    "\n",
    "En vez de crear las funciones para entrenar nuestra red neuronal, utilizaremos Keras con Tensorflow para hacer el modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnistnn.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Obtener las dimensiones de los datos de entrenamiento\n",
    "print('Datos de entrenamiento:', x_train.shape)\n",
    "print('Datos de prueba:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ver un ejemplo\n",
    "print('El número es un :', y_train[5])\n",
    "sns.set_style('whitegrid')\n",
    "plt.imshow(x_train[5,:,:], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, la red neuronal que hemos visto hasta ahora no acepta imágenes como datos de entrada. Para ello, tenemos que convertir las matrices que representan la imagen a vectores. Además, como queremos hacer una clasificación de más de dos categorías, hay que codificar las etiquetas con un one-hot-encoding.\n",
    "\n",
    "Esto último lo podemos hacer con pandas, scikit learn o el mismo keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One hot encoding\n",
    "print(y_train[:5])\n",
    "\n",
    "num_classes = 10\n",
    "y_train = \n",
    "y_test  = \n",
    "\n",
    "# Ver los primeros 5 renglones\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de entrada deben ser vectores, entonces convertimos las matrices con reshape.\n",
    "image_size = 28*28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size)\n",
    "x_test  = x_test.reshape(x_test.shape[0], image_size)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los valores de la imágen de 0 a 255, a 0 a 1\n",
    "x_train = \n",
    "x_test  = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente cuando trabajamos con ML hay que ajustar los datos a que todos tengan la misma escala. \n",
    "\n",
    "Las imágenes realmente son una matriz de números de 0 a 255. En una imagen en blanco y negro, 0 representa el color negro y 255 el color blanco. Para ajustar la escala en las imágenes, lo más común es dividir los valores que hay en las imágenes entre 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una red neuronal secuencial, lo que quiere decir que por el momento ningún nodo va a proporcionar retroalimentación a nodos en capas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crear modelo secuencial con capas densas\n",
    "from keras.layers import Dense      # Dense layers are \"fully connected\" layers\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/\n",
    "\n",
    "image_size  = 28*28   # Tamaño de las imágenes\n",
    "classes     = 10      # 10 dígitos\n",
    "\n",
    "## Creamos una red neuronal secuencial\n",
    "redNeuronal = Sequential()\n",
    "\n",
    "## La primera capa la vamos a crear con 32 nodos. Como es la primera capa, hay que especificar que va a recibir\n",
    "## datos del tamaño de 784 (el resultado de 28*28)\n",
    "redNeuronal.add(Dense(units=   , activation=   , input_shape=      ))\n",
    "\n",
    "## La segunda capa va a ser la de salida. Como es una clasificación multicategoría, la función de activación \n",
    "## debe ser softmax\n",
    "redNeuronal.add(Dense(units=   , activation=    ))\n",
    "\n",
    "## Resumen del modelo\n",
    "redNeuronal.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `Softmax` es una generalización de la entropía cruzada cuando hay más de dos categorías. Esta función de costo se relaciona con las probabilidades de que el dígito sea reconocido entre el 0, 1, 2, ... , 9. Cada nodo de la última capa corresponde a un dígito. El nodo con el valor de probabilidad más grande es el que decide la categoría a asignar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que llenamos los hiperparámetros, hay que compilar el modelo.\n",
    "# Vamos a usar el stochastic gradient descent (mini batch), con crossentropy y para probar el modelo\n",
    "# utilizaremos la precisión\n",
    "redNeuronal.compile(optimizer=  , loss=     , metrics=[    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTRENAMIENTO\n",
    "## Queremos que el entrenamiento sea con lotes de tamaño 128, que sean en total 5 épocas y que\n",
    "## Para probar el modelo utilice el 10% de los datos de entrenamiento.\n",
    "history = redNeuronal.fit(x_train, y_train, batch_size=     , epochs=     , verbose=False, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRUEBAS\n",
    "# Obtenemos los datos de la función de costo y la precisión del modelo con evaluate.\n",
    "loss, accuracy  = redNeuronal.evaluate(x_test, y_test, verbose=False)\n",
    "\n",
    "## Graficar los resultados\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Precisión del modelo durante entrenamiento')\n",
    "plt.ylabel('Precisión')\n",
    "plt.xlabel('Época')\n",
    "plt.legend(['Entrenamiento', 'Validación'], loc='best')\n",
    "\n",
    "print(f'Precisión en datos de prueba: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vamos a probar la predicción.\n",
    "# Escogemos un valor al azar\n",
    "index   = np.random.choice(x_test.shape[0])\n",
    "x_index = x_test[index,:].reshape(1,-1)\n",
    "prediccion = np.squeeze(redNeuronal.predict(x_index))\n",
    "\n",
    "# El vector de predicción contiene los valores de las probabilidades de que sea uno de los dígitos de (0,1,2,...,8,9) en ese\n",
    "# orden. Para encontrar el valor predicho hay que buscar la POSICIÓN con el valor más alto.\n",
    "max_index = np.squeeze(np.where(prediccion==prediccion.max()))\n",
    "\n",
    "# Mostrar el resultado\n",
    "print('El valor predicho es:', max_index, '\\n')\n",
    "print('El valor real es:')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.imshow(np.reshape(x_test[index,:],(28,28)), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear unas funciones para llamar al modelo anterior de manera más rápida. Además, vamos a probar con diferente número de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crear modelo secuencial\n",
    "def createModel(layer_sizes, input_shape, num_classes, activation = 'sigmoid', output_activation='softmax'):\n",
    "    # Inicializar modelo\n",
    "    model = Sequential()\n",
    "    # Agregar capa de entrada\n",
    "    model.add(Dense(layer_sizes[0], activation=activation, input_shape=(input_shape,)))\n",
    "    \n",
    "    if(len(layer_sizes)>1):\n",
    "        for s in layer_sizes[1:]:\n",
    "            model.add(Dense(units = s, activation = activation))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation = output_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y evaluar\n",
    "def evaluateModel(model,  x_train, y_train, x_test, y_test,\n",
    "                  batch_size=128, epochs=5, optimizer_func = \"sgd\", loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'], validation_split=0.1):\n",
    "    # Show model summary\n",
    "    model.summary()\n",
    "    model.compile(optimizer=optimizer_func, loss=loss, metrics=metrics)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                        validation_split=validation_split, verbose=False)\n",
    "    loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
    "    # Graficar resultados\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Precisión del modelo durante entrenamiento')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Precisión en datos de prueba: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir\n",
    "def predictModel(model, x_test, index=50):\n",
    "    x_index = x_test[index,:].reshape(1,-1)\n",
    "    prediccion = np.squeeze(redNeuronal.predict(x_index))\n",
    "    # Encontrar el valor máximo\n",
    "    max_index = np.squeeze(np.where(prediccion==prediccion.max()))\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    print('El valor predicho es:', max_index, '\\n')\n",
    "    print('El valor real es:')\n",
    "\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.imshow(np.reshape(x_test[index,:],(28,28)), cmap='Greys')\n",
    "    plt.show()\n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redNeuronal2 = createModel([32, 16], 28*28, 10)\n",
    "evaluateModel(redNeuronal2,  x_train, y_train, x_test, y_test, epochs=20)\n",
    "predictModel(redNeuronal2, x_test, index=9973)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales (CNN o ConvNet en inglés)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien los datos MNIST son imágenes, la resolución de cada una de ellas es muy baja (28x28 pixeles). Además, todas las imágenes están en escala de grises. Esto no sucede con imágenes del día a día. Si quisieramos utilizar una red neuronal en imágenes en 4K Ultra HD, sólo la capa de entrada requeriría un tamaño de 26,542,080 nodos (4096 x 2160 x 3 [canales RGB]).\n",
    "\n",
    "Además, las redes neuronales convencionales requieren que la imagen sea transformada a un vector,lo que hace que se pierda cierta información espacial y de correlación entre pixeles. Entonces, en vez de utilizar la información de cada uno de los pixeles por separado, sería más eficiente ver una imagen por 'zonas'. Esta idea es la base para las redes neuronales convolucionales.\n",
    "\n",
    "La idea detrás de las CNN es similar a cómo reconocemos nosotros las imágenes. Por ejemplo, podemos identificar el rostro de una persona por la nariz, los ojos, el cabello, la boca, etc. Pero no sólo nos limitamos a ver si estos elementos están presentes, la forma y la posición de cada uno de ellos también son elementos que nos ayudan a identificar el rostro. Las CNN utilizan información local de ciertas zonas de las imágenes para poder construir patrones cada vez más complejos, ayudando a determinar estructuras más complejas confome los datos pasan a través de las capas.\n",
    "\n",
    "<img src=\"images/featurescomplex.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas de convolución\n",
    "\n",
    "Las CNN utilizan capas de 'filtros' que van recorriendo toda la imagen y comprimen la información de esas regiones. Estos filtros son como una pequeña ventana que se desliza a través de la imagen y van calculando un resultado.\n",
    "\n",
    "<img src=\"images/cnnwindow.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "El resultado de aplicar el filtro por todas las posiciones posibles de la imagen da como resultado las entradas que las neuronas de la siguiente capa deben recibir, sólo que ahora las neuronas están ordenadas como una matriz y no como un vector. Cada neurona de la siguiente capa recibe sólo un valor, por lo que a los datos que están dentro de cada ventana se les aplica una función de activación tal y como en una red tradicional.\n",
    "\n",
    "<img src=\"images/cnnwindow2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Un filtro descubre una característica en particular de la imagen (un trazo, línea), pero para poder describir completamente la imagen requerimos de más de una característica. Es por esto que se utilizan varios filtros por cada ca\n",
    "\n",
    "<img src=\"images/cnnfilter.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas de reducción por muestreo (pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de calcular los filtros, comúnmente se realiza una operación más para reducir aún más la cantidad de datos que se tiene que procesar dentro de la red neuronal. A este paso se le conoce como reducción por muestreo (**pooling** en inglés). El *pooling* es como otra ventana por la capa de los filtros, pero ahora en vez de calcular pesos va a calcular el valor máximo, el promedio o la suma de todos los elementos que estén en la ventana.\n",
    "\n",
    "<img src=\"images/pooling.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Básicamente, el pooling lo que hace es crear una imagen más pequeña pero con información suficiente para poder detectar los rasgos importantes.\n",
    "\n",
    "<img src=\"images/poolingexample.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso completo en una CNN\n",
    "\n",
    "Después de aplicar algunas capas de convolución y de pooling, después procedemos como si tuvieramos una red neuronal tradicional: los datos se convierten a vectores, se calculan pesos y se aplican las funciones de activación.\n",
    "\n",
    "<img src=\"images/cnncomplete.jpeg\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación del MNIST con CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuevamente los datos para tener las formar originales.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Vemos otra vez la forma de los datos\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando utilizamos redes convolucionales, los datos de entrada deben tener 3 dimensiones: largo, ancho y espesor.\n",
    "\n",
    "De los datos de arriba, vemos que hay 60,000 imágenes de 28x28 en los datos de entrenamiento, falta la dimensión de espesor (que en este caso es 1).\n",
    "\n",
    "Hay que convertir los datos a la forma correcta utilizando np.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a 3 dimensiones las imágenes\n",
    "x_train = \n",
    "x_test  = \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Guardar la forma de los datos de entrada\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos la escala a 0 a 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train = \n",
    "x_test  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos a one.hot encoding las variables de etiqueta\n",
    "num_classes = 10\n",
    "y_train = \n",
    "y_test  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las funciones necesarias de Keras.\n",
    "# Una capa de convolución usamos Conv2D\n",
    "# Una capa de pooling (con la operación max) la usamos como MaxPooling2D\n",
    "# Para convertir el resultado de la convolución y pooling a un vector, usamos Flatten\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "# Creamos un modelo secuencial como en el ejemplo anterior\n",
    "redConv = Sequential()\n",
    "\n",
    "# Agregamos una capa de 32 filtros de 5x5 con función de activación ReLu\n",
    "redConv.add(Conv2D(   ,   ,activation=    , input_shape=    ))\n",
    "# Agregamos una capa de pooling de 2x2\n",
    "redConv.add(MaxPooling2D(     ))\n",
    "\n",
    "## Agregamos una segunda capa de 64 filtros de 5x5\n",
    "redConv.add(Conv2D(     ,     , activation=    ))\n",
    "# Agregamos una capa de pooling de 2x2\n",
    "redConv.add(MaxPooling2D(    ))\n",
    "\n",
    "## Para convertir la capa anterior a un vector, usamos flatten\n",
    "redConv.add(Flatten())\n",
    "\n",
    "## Como queremos clasificar en muchas categorías, usamos softmax como función de activación en la capa de salida\n",
    "## El tipo de capa es Dense como en una red normal.\n",
    "redConv.add(Dense(     , activation=     ))\n",
    "\n",
    "## Mostrar el resumen \n",
    "redConv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamos el modelo\n",
    "redConv.compile(loss=     , optimizer=     ,metrics=[    ])\n",
    "\n",
    "history = redConv.fit(x_train, y_train, batch_size=     , epochs=   , validation_split=0.1, verbose=False)\n",
    "loss, accuracy  = redConv.evaluate(x_test, y_test, verbose=False)\n",
    "\n",
    "# Graficar resultados\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Precisión del modelo durante entrenamiento')\n",
    "plt.ylabel('Precisión')\n",
    "plt.xlabel('Época')\n",
    "plt.legend(['Entrenamiento', 'Validación'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(f'Precisión en datos de prueba: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index   = np.random.choice(x_test.shape[0])\n",
    "x_index = x_test[index,:].reshape(1,28,28,1)\n",
    "prediccion = np.squeeze(redConv.predict(x_index))\n",
    "\n",
    "# El vector de predicción contiene los valores de las probabilidades de que sea uno de los dígitos de (0,1,2,...,8,9) en ese\n",
    "# orden. Para encontrar el valor predicho hay que buscar la POSICIÓN con el valor más alto.\n",
    "max_index = np.squeeze(np.where(prediccion==prediccion.max()))\n",
    "\n",
    "# Mostrar el resultado\n",
    "print('El valor predicho es:', max_index, '\\n')\n",
    "print('El valor real es:')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.imshow(np.reshape(x_test[index,:],(28,28)), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo anterior tarda mucho en compilar. Vamos a crear un modelo más chico y además vamos a probar otro optimizador.\n",
    "\n",
    "redConv2 = Sequential()\n",
    "redConv2.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "redConv2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "redConv2.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "redConv2.add(Dense(128, activation='relu'))\n",
    "redConv2.add(Dense(10,activation='softmax'))\n",
    "\n",
    "redConv2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "redConv2.fit(x=x_train,y=y_train, epochs=3)\n",
    "\n",
    "redConv2.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "index= 7831\n",
    "x_index = x_test[index,:].reshape(1,28,28,1)\n",
    "prediccion = np.squeeze(redConv2.predict(x_index))\n",
    "\n",
    "# El vector de predicción contiene los valores de las probabilidades de que sea uno de los dígitos de (0,1,2,...,8,9) en ese\n",
    "# orden. Para encontrar el valor predicho hay que buscar la POSICIÓN con el valor más alto.\n",
    "max_index = np.squeeze(np.where(prediccion==prediccion.max()))\n",
    "\n",
    "# Mostrar el resultado\n",
    "print('El valor predicho es:', max_index, '\\n')\n",
    "print('El valor real es:')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.imshow(np.reshape(x_test[index,:],(28,28)), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "1. [©\tMIT 6.S191: Introduction to Deep Learning.](introtodeeplearning.com)\n",
    "2. [A Brief Introduction to Deep Learning](http://www.cs.tau.ac.il/~dcor/Graphics/pdf.slides/YY-Deep%20Learning.pdf)\n",
    "3. [The Intellectual Excitement of Computer Science](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/index.html)\n",
    "4. [Adventures in Machine Learning](https://adventuresinmachinelearning.com/wp-content/uploads/2017/07/An-introduction-to-neural-networks-for-beginners.pdf)\n",
    "5. [A gentle introduction to neural networks](https://www.pycon.it/media/conference/slides/a-gentle-introduction-to-neural-networks-with-python.pdf)\n",
    "6. [Machine Learning for Beginners: An Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/)\n",
    "7. [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/about.html)\n",
    "8. [A Beginner's Guide to Neural Networks and Deep Learning](https://skymind.ai/wiki/neural-network)\n",
    "9. [The deep learning book](http://www.deeplearningbook.org/)\n",
    "10. [Deep Learning: Introducción práctica con Keras](https://torres.ai/deeplearning/)\n",
    "11. [Convolutional neural networks for beginners](https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca)\n",
    "12. [Image Classification in 10 minutes](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
